{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83dd6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark DataSkew -Salting\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','2'). \\\n",
    "    config('spark.executor.memory','512MB'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52ed11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc517d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_11844'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4875de67",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9579248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee Skew data\n",
    "schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(\"Datasets/employee_records_skew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32fb5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "| first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|      Jacob|     Stark|         Fine artist|1976-04-25|jasonortiz@exampl...|  224-695-9516x02171|358889.0|            1|\n",
      "|    Marissa|     Crane|Intelligence analyst|2000-06-24|johnsontroy@examp...|        277-928-0029|786608.0|            3|\n",
      "|     Andrea|     Davis|     Physiotherapist|1999-06-17| ihowell@example.org|          9503082950|428991.0|            3|\n",
      "|       John|     Tapia|Lecturer, further...|2001-09-23|russobarbara@exam...|    001-679-487-9525|241574.0|            9|\n",
      "|      Colin|    Holmes|     Psychotherapist|1965-06-29|fsimmons@example.org|          3232202899|320260.0|            4|\n",
      "|       Eric|      Beck|Speech and langua...|1970-10-28|johnny90@example.net|+1-744-277-5637x0...|125977.0|            4|\n",
      "|      Jason|    Robles|Sound technician,...|1967-03-21|jessicathomas@exa...|    001-872-480-5161|447336.0|            2|\n",
      "|   Courtney|    Daniel|     Careers adviser|1994-11-14| gmurray@example.org|  (415)357-1570x5493|514742.0|            2|\n",
      "|  Stephanie|   Morales|         Pathologist|1982-09-24|michael68@example...|001-311-431-6117x...| 14207.0|            6|\n",
      "|   Virginia| Cervantes|Chief Operating O...|1994-12-10|nicholasdavis@exa...|     +1-807-442-2027|756630.0|           10|\n",
      "|     Elijah|    Glover|Therapist, speech...|1969-10-20|brookefrederick@e...|        434.735.9681|611461.0|            8|\n",
      "|   Jennifer|   Collins|       Site engineer|1975-09-11|joseph06@example.org|     +1-416-797-0488| 19458.0|            4|\n",
      "|  Christina|      Ball|Teacher, special ...|1962-12-20| fwatson@example.org|    280.582.7025x086|709984.0|            9|\n",
      "|       Drew|  Erickson|Psychologist, edu...|1996-11-05| jason18@example.com|001-271-662-7827x...|304998.0|            3|\n",
      "|      James|    Wright|Air traffic contr...|1975-10-21|   arice@example.com|       (721)398-6804|403612.0|            4|\n",
      "|    Valerie|     Smith|      Phytotherapist|1985-04-14|frederickjames@ex...|   498.939.5081x5501|181849.0|            6|\n",
      "|Christopher|     Smith|Accountant, chart...|1970-04-01|robert20@example.net|        559-309-6778| 96578.0|            1|\n",
      "|    Patrick|     Bowen| Exhibition designer|1968-01-02|   tbell@example.net|   863-759-4636x5870|276783.0|            6|\n",
      "|      Erika|Cunningham|           Mudlogger|1974-07-02|lauren19@example.com|    688-359-8811x177|647089.0|            6|\n",
      "|    Melissa|    Murray| Librarian, academic|1985-06-14|christinaruiz@exa...|        841.594.5213| 48231.0|            5|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42df3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept data\n",
    "dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(dept_schema).option(\"header\", True).load(\"Datasets/department_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "406d407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increaseing dataset size\n",
    "emp_df = emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24547e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200365"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19d5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "df_joined = emp_df.join(dept, on=emp_df.department_id==dept.department_id, how=\"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2368c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "875b3fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortMergeJoin [department_id#839], [department_id#848], LeftOuter\n",
      ":- *(1) Sort [department_id#839 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(department_id#839, 200), ENSURE_REQUIREMENTS, [id=#836]\n",
      ":     +- FileScan csv [first_name#832,last_name#833,job_title#834,dob#835,email#836,phone#837,salary#838,department_id#839] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv018960/Datasets/employee_records_skew.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- *(3) Sort [department_id#848 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(department_id#848, 200), ENSURE_REQUIREMENTS, [id=#847]\n",
      "      +- *(2) Filter isnotnull(department_id#848)\n",
      "         +- FileScan csv [department_id#848,department_name#849,description#850,city#851,state#852,country#853] Batched: false, DataFilters: [isnotnull(department_id#848)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv018960/Datasets/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d3a66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|partition_num| count|\n",
      "+-------------+------+\n",
      "|          103|100417|\n",
      "|          122| 99780|\n",
      "|           43| 99451|\n",
      "|          107| 99805|\n",
      "|           49| 99706|\n",
      "|           51|100248|\n",
      "|          102|100214|\n",
      "|           66|200420|\n",
      "|          174|200310|\n",
      "|           89|100014|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition count\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit, desc\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "293cc142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|department_id|count(1)|\n",
      "+-------------+--------+\n",
      "|            1|   99451|\n",
      "|            6|   99706|\n",
      "|            3|  100248|\n",
      "|            5|  200420|\n",
      "|            9|  100014|\n",
      "|            4|  100214|\n",
      "|            8|  100417|\n",
      "|            7|   99805|\n",
      "|           10|   99780|\n",
      "|            2|  200310|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp_df.groupBy(\"department_id\").agg(count(lit(1))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5502d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 48)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "580533a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#salting\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 48)\n",
    "\n",
    "# Salt Data Frame to add to department\n",
    "salt_df = spark.range(0, 48)\n",
    "salt_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc1b5d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
      "| first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|salted_dept_id|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
      "|      Jacob|     Stark|         Fine artist|1976-04-25|jasonortiz@exampl...|  224-695-9516x02171|358889.0|            1|          1_30|\n",
      "|    Marissa|     Crane|Intelligence analyst|2000-06-24|johnsontroy@examp...|        277-928-0029|786608.0|            3|          3_46|\n",
      "|     Andrea|     Davis|     Physiotherapist|1999-06-17| ihowell@example.org|          9503082950|428991.0|            3|          3_31|\n",
      "|       John|     Tapia|Lecturer, further...|2001-09-23|russobarbara@exam...|    001-679-487-9525|241574.0|            9|          9_15|\n",
      "|      Colin|    Holmes|     Psychotherapist|1965-06-29|fsimmons@example.org|          3232202899|320260.0|            4|          4_26|\n",
      "|       Eric|      Beck|Speech and langua...|1970-10-28|johnny90@example.net|+1-744-277-5637x0...|125977.0|            4|          4_15|\n",
      "|      Jason|    Robles|Sound technician,...|1967-03-21|jessicathomas@exa...|    001-872-480-5161|447336.0|            2|          2_28|\n",
      "|   Courtney|    Daniel|     Careers adviser|1994-11-14| gmurray@example.org|  (415)357-1570x5493|514742.0|            2|          2_18|\n",
      "|  Stephanie|   Morales|         Pathologist|1982-09-24|michael68@example...|001-311-431-6117x...| 14207.0|            6|          6_37|\n",
      "|   Virginia| Cervantes|Chief Operating O...|1994-12-10|nicholasdavis@exa...|     +1-807-442-2027|756630.0|           10|         10_44|\n",
      "|     Elijah|    Glover|Therapist, speech...|1969-10-20|brookefrederick@e...|        434.735.9681|611461.0|            8|           8_9|\n",
      "|   Jennifer|   Collins|       Site engineer|1975-09-11|joseph06@example.org|     +1-416-797-0488| 19458.0|            4|          4_25|\n",
      "|  Christina|      Ball|Teacher, special ...|1962-12-20| fwatson@example.org|    280.582.7025x086|709984.0|            9|          9_24|\n",
      "|       Drew|  Erickson|Psychologist, edu...|1996-11-05| jason18@example.com|001-271-662-7827x...|304998.0|            3|          3_37|\n",
      "|      James|    Wright|Air traffic contr...|1975-10-21|   arice@example.com|       (721)398-6804|403612.0|            4|          4_44|\n",
      "|    Valerie|     Smith|      Phytotherapist|1985-04-14|frederickjames@ex...|   498.939.5081x5501|181849.0|            6|           6_7|\n",
      "|Christopher|     Smith|Accountant, chart...|1970-04-01|robert20@example.net|        559-309-6778| 96578.0|            1|          1_37|\n",
      "|    Patrick|     Bowen| Exhibition designer|1968-01-02|   tbell@example.net|   863-759-4636x5870|276783.0|            6|          6_33|\n",
      "|      Erika|Cunningham|           Mudlogger|1974-07-02|lauren19@example.com|    688-359-8811x177|647089.0|            6|          6_41|\n",
      "|    Melissa|    Murray| Librarian, academic|1985-06-14|christinaruiz@exa...|        841.594.5213| 48231.0|            5|           5_0|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Salted to EMP tables\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))\n",
    "\n",
    "salted_emp.show()                                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7da04516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200365"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c5688e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200365"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salted_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd551a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------------------+---------+-----+-------+---+--------------+\n",
      "|department_id|department_name|         description|     city|state|country| id|salted_dept_id|\n",
      "+-------------+---------------+--------------------+---------+-----+-------+---+--------------+\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  0|           5_0|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  1|           5_1|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  2|           5_2|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  3|           5_3|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  4|           5_4|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  5|           5_5|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  6|           5_6|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  7|           5_7|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  8|           5_8|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji|  9|           5_9|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 10|          5_10|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 11|          5_11|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 12|          5_12|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 13|          5_13|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 14|          5_14|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 15|          5_15|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 16|          5_16|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 17|          5_17|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 18|          5_18|\n",
      "|            5|     Hardin Inc|Re-contextualized...|Hayestown|   WA|   Fiji| 19|          5_19|\n",
      "+-------------+---------------+--------------------+---------+-----+-------+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Department\n",
    "\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
    "\n",
    "salted_dept.where(\"department_id = 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c79ebf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "532572ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salted_dept.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4faea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "\n",
    "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5a6ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write Action\n",
    "\n",
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20cb92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|           18|36727|\n",
      "|           38|32467|\n",
      "|           31|24775|\n",
      "|            3|24379|\n",
      "|           27|20414|\n",
      "|           37|24421|\n",
      "|            2|14304|\n",
      "|           22|20493|\n",
      "|            9|36516|\n",
      "|           17|22497|\n",
      "|           11|38636|\n",
      "|           25|28739|\n",
      "|           19|20210|\n",
      "|           30|34866|\n",
      "|           40|22485|\n",
      "|           42|16314|\n",
      "|           28|14317|\n",
      "|           36|29006|\n",
      "|            0|26354|\n",
      "|            8|28690|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the each partition counts\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e775bfc",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. understand data skew\n",
    "2. understand data spill to memory and Disk\n",
    "3. how to prepare the salted key\n",
    "4. how to reduce spills using salting approch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
