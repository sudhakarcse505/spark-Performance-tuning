{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65d33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark dataSkew Window Rank\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','5'). \\\n",
    "    config('spark.executor.memory','512MB'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8eb55550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830151ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_18473'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50476f25",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable AQE and AdvisoryPartitionSize\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c720cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee Skew data\n",
    "schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(\"Datasets/employee_records_skew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651571ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "| first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|      Jacob|     Stark|         Fine artist|1976-04-25|jasonortiz@exampl...|  224-695-9516x02171|358889.0|            1|\n",
      "|    Marissa|     Crane|Intelligence analyst|2000-06-24|johnsontroy@examp...|        277-928-0029|786608.0|            3|\n",
      "|     Andrea|     Davis|     Physiotherapist|1999-06-17| ihowell@example.org|          9503082950|428991.0|            3|\n",
      "|       John|     Tapia|Lecturer, further...|2001-09-23|russobarbara@exam...|    001-679-487-9525|241574.0|            9|\n",
      "|      Colin|    Holmes|     Psychotherapist|1965-06-29|fsimmons@example.org|          3232202899|320260.0|            4|\n",
      "|       Eric|      Beck|Speech and langua...|1970-10-28|johnny90@example.net|+1-744-277-5637x0...|125977.0|            4|\n",
      "|      Jason|    Robles|Sound technician,...|1967-03-21|jessicathomas@exa...|    001-872-480-5161|447336.0|            2|\n",
      "|   Courtney|    Daniel|     Careers adviser|1994-11-14| gmurray@example.org|  (415)357-1570x5493|514742.0|            2|\n",
      "|  Stephanie|   Morales|         Pathologist|1982-09-24|michael68@example...|001-311-431-6117x...| 14207.0|            6|\n",
      "|   Virginia| Cervantes|Chief Operating O...|1994-12-10|nicholasdavis@exa...|     +1-807-442-2027|756630.0|           10|\n",
      "|     Elijah|    Glover|Therapist, speech...|1969-10-20|brookefrederick@e...|        434.735.9681|611461.0|            8|\n",
      "|   Jennifer|   Collins|       Site engineer|1975-09-11|joseph06@example.org|     +1-416-797-0488| 19458.0|            4|\n",
      "|  Christina|      Ball|Teacher, special ...|1962-12-20| fwatson@example.org|    280.582.7025x086|709984.0|            9|\n",
      "|       Drew|  Erickson|Psychologist, edu...|1996-11-05| jason18@example.com|001-271-662-7827x...|304998.0|            3|\n",
      "|      James|    Wright|Air traffic contr...|1975-10-21|   arice@example.com|       (721)398-6804|403612.0|            4|\n",
      "|    Valerie|     Smith|      Phytotherapist|1985-04-14|frederickjames@ex...|   498.939.5081x5501|181849.0|            6|\n",
      "|Christopher|     Smith|Accountant, chart...|1970-04-01|robert20@example.net|        559-309-6778| 96578.0|            1|\n",
      "|    Patrick|     Bowen| Exhibition designer|1968-01-02|   tbell@example.net|   863-759-4636x5870|276783.0|            6|\n",
      "|      Erika|Cunningham|           Mudlogger|1974-07-02|lauren19@example.com|    688-359-8811x177|647089.0|            6|\n",
      "|    Melissa|    Murray| Librarian, academic|1985-06-14|christinaruiz@exa...|        841.594.5213| 48231.0|            5|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e579219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the skew for department_id=2\n",
    "emp1 = emp.drop(\"job_title\",\"email\",\"phone\")\n",
    "\n",
    "emp_df = emp1 #.union(emp1.where(\"department_id == 2\")).union(emp1.where(\"department_id == 2\")).union(emp1.where(\"department_id == 2\")) \\\n",
    "             #.union(emp1.where(\"department_id == 2\")).union(emp1.where(\"department_id == 2\")).union(emp1.where(\"department_id == 2\"))\n",
    "            # .union(emp.where(\"department_id == 2\")).union(emp.where(\"department_id == 2\")).union(emp.where(\"department_id == 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8facef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, avg, sum, min, max, rank, desc\n",
    "\n",
    "# Window function: rank events by date per user\n",
    "window_spec = Window.partitionBy(\"department_id\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df_with_rank = emp_df.withColumn(\"rank\", rank().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fdc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to noop - takes 8 Sec\n",
    "\n",
    "df_with_rank.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3175ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please check the Spark Ui -> SQL -> Job -> Stage -> Task details\n",
    "#Observe the dataSkew Issues\n",
    "#Observe the DataSpill Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106ed635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|partition_num| count|\n",
      "+-------------+------+\n",
      "|           10|  8713|\n",
      "|            9| 60703|\n",
      "|            8| 65250|\n",
      "|            7| 65699|\n",
      "|            6|217684|\n",
      "|            5|130421|\n",
      "|            4|130312|\n",
      "|            3|130393|\n",
      "|            2|130400|\n",
      "|            1|130384|\n",
      "|            0|130406|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition count\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit, desc\n",
    "\n",
    "part_df = emp_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.orderBy(desc(\"partition_num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130cc954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|department_id|count(1)|\n",
      "+-------------+--------+\n",
      "|            1|   99451|\n",
      "|            6|   99706|\n",
      "|            3|  100248|\n",
      "|            5|  200420|\n",
      "|            9|  100014|\n",
      "|            4|  100214|\n",
      "|            8|  100417|\n",
      "|            7|   99805|\n",
      "|           10|   99780|\n",
      "|            2|  200310|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp_df.groupBy(\"department_id\").agg(count(lit(1))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74654128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count how many employees are at each salary level in each department\n",
    "\n",
    "from pyspark.sql.functions import count, lit, desc, col,desc, sum, count, coalesce\n",
    "\n",
    "salary_counts = emp_df.groupBy(\"department_id\", \"salary\") \\\n",
    "                  .agg(count(\"*\").alias(\"employee_count\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6e14283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------------+----+\n",
      "|department_id|  salary|employee_count|rank|\n",
      "+-------------+--------+--------------+----+\n",
      "|            1|999996.0|             1|   1|\n",
      "|            1|999979.0|             1|   2|\n",
      "|            1|999959.0|             1|   3|\n",
      "|            1|999922.0|             1|   4|\n",
      "|            1|999917.0|             1|   5|\n",
      "|            1|999914.0|             1|   6|\n",
      "|            1|999911.0|             1|   7|\n",
      "|            1|999901.0|             2|   8|\n",
      "|            1|999896.0|             1|  10|\n",
      "|            1|999894.0|             1|  11|\n",
      "|            1|999885.0|             1|  12|\n",
      "|            1|999879.0|             1|  13|\n",
      "|            1|999863.0|             1|  14|\n",
      "|            1|999849.0|             1|  15|\n",
      "|            1|999846.0|             1|  16|\n",
      "|            1|999844.0|             1|  17|\n",
      "|            1|999843.0|             2|  18|\n",
      "|            1|999829.0|             1|  20|\n",
      "|            1|999815.0|             1|  21|\n",
      "|            1|999809.0|             1|  22|\n",
      "+-------------+--------+--------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute rank by summing employee counts of higher salaries (descending order)\n",
    "window_desc = Window.partitionBy(\"department_id\") \\\n",
    "                    .orderBy(col(\"salary\").desc()) \\\n",
    "                    .rowsBetween(Window.unboundedPreceding, -1)  # exclude current row\n",
    "\n",
    "salary_ranks = salary_counts.withColumn(\n",
    "    \"rank\", coalesce(sum(coalesce(\"employee_count\")).over(window_desc),lit(0)) + 1\n",
    ")\n",
    "\n",
    "salary_ranks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "476e09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Join back to original data to assign the rank\n",
    "\n",
    "final_df = emp_df.join(salary_ranks, on=[\"department_id\", \"salary\"], how=\"left\") \\\n",
    "             .select(\"*\") \\\n",
    "             .orderBy(\"department_id\", \"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb87014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to noop - takes 8 Sec\n",
    "\n",
    "final_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "852d5d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+---------+----------+--------------+----+\n",
      "|department_id|  salary|first_name|last_name|       dob|employee_count|rank|\n",
      "+-------------+--------+----------+---------+----------+--------------+----+\n",
      "|            2|999997.0|   Richard|   Foster|1990-03-26|             2|   1|\n",
      "|            2|999997.0|   Richard|   Foster|1990-03-26|             2|   1|\n",
      "|            2|999987.0|   Cynthia|    Lewis|1994-05-15|             2|   3|\n",
      "|            2|999987.0|   Cynthia|    Lewis|1994-05-15|             2|   3|\n",
      "|            2|999964.0|    Melody| Reynolds|1987-09-11|             2|   5|\n",
      "|            2|999964.0|    Melody| Reynolds|1987-09-11|             2|   5|\n",
      "|            2|999903.0|    Thomas|   Medina|1997-08-10|             2|   7|\n",
      "|            2|999903.0|    Thomas|   Medina|1997-08-10|             2|   7|\n",
      "|            2|999885.0|    Andrea|   Gordon|1998-06-07|             2|   9|\n",
      "|            2|999885.0|    Andrea|   Gordon|1998-06-07|             2|   9|\n",
      "+-------------+--------+----------+---------+----------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#some sample data with row_number\n",
    "\n",
    "final_df.where(\"department_id == 2\").where(\"rank < 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90edf12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+-------------+----+\n",
      "|first_name|last_name|       dob|  salary|department_id|rank|\n",
      "+----------+---------+----------+--------+-------------+----+\n",
      "|   Richard|   Foster|1990-03-26|999997.0|            2|   1|\n",
      "|   Richard|   Foster|1990-03-26|999997.0|            2|   1|\n",
      "|   Cynthia|    Lewis|1994-05-15|999987.0|            2|   3|\n",
      "|   Cynthia|    Lewis|1994-05-15|999987.0|            2|   3|\n",
      "|    Melody| Reynolds|1987-09-11|999964.0|            2|   5|\n",
      "|    Melody| Reynolds|1987-09-11|999964.0|            2|   5|\n",
      "|    Thomas|   Medina|1997-08-10|999903.0|            2|   7|\n",
      "|    Thomas|   Medina|1997-08-10|999903.0|            2|   7|\n",
      "|    Andrea|   Gordon|1998-06-07|999885.0|            2|   9|\n",
      "|    Andrea|   Gordon|1998-06-07|999885.0|            2|   9|\n",
      "+----------+---------+----------+--------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#some sample data with row_number\n",
    "\n",
    "df_with_rank.where(\"department_id == 2\").where(\"rank < 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62292a5b",
   "metadata": {},
   "source": [
    "✅ Summary:\n",
    "With this approch we were able to solve this problem with 60% less resources and improved Job execution time by 80%\n",
    "If you try with larger datasets, you will definatly see performance improvements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
