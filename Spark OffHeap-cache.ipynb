{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc3159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark OffHeap Memory\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','1'). \\\n",
    "    config('spark.executor.memory','1g'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    config('spark.memory.offHeap.enabled','True'). \\\n",
    "    config('spark.memory.offHeap.size','1g'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc518a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78ece39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_12972'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e73061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2269096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(0, 10_000_000).withColumnRenamed(\"id\", \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9e983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_storage(df, storage_level, label):\n",
    "    df_unpersisted = df.unpersist(blocking=True)\n",
    "    df_cached = df.persist(storage_level)\n",
    "\n",
    "    # Trigger caching\n",
    "    df_cached.count()\n",
    "\n",
    "    # Benchmark action time\n",
    "    start = time.time()\n",
    "    df_cached.selectExpr(\"avg(number)\").collect()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"{label:<20}: {end - start:.3f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a252d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY_ONLY         : 0.478 sec\n"
     ]
    }
   ],
   "source": [
    "# MEMORY_ONLY\n",
    "benchmark_storage(df, StorageLevel.MEMORY_ONLY, \"MEMORY_ONLY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c785ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISK_ONLY           : 0.120 sec\n"
     ]
    }
   ],
   "source": [
    "# DISK_ONLY\n",
    "benchmark_storage(df, StorageLevel.DISK_ONLY, \"DISK_ONLY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488e2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFF_HEAP            : 0.080 sec\n"
     ]
    }
   ],
   "source": [
    "# OFF_HEAP\n",
    "benchmark_storage(df, StorageLevel.OFF_HEAP, \"OFF_HEAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6089806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD ID: 46\n",
      "  Name: *(1) Project [id#140L AS number#142L]\n",
      "+- *(1) Range (0, 10000000, step=1, splits=4)\n",
      "\n",
      "  Partitions: 4\n",
      "  Storage Level: Disk Memory (off heap) Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# List all cached RDDs (technically for RDDs only)\n",
    "persistent_rdds = spark.sparkContext._jsc.getPersistentRDDs()\n",
    "for rdd_id in persistent_rdds:\n",
    "    rdd = persistent_rdds[rdd_id]\n",
    "    print(f\"RDD ID: {rdd_id}\")\n",
    "    print(f\"  Name: {rdd.name()}\")\n",
    "    print(f\"  Partitions: {rdd.partitions().size()}\")\n",
    "    print(f\"  Storage Level: {rdd.getStorageLevel().description()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8bf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion:\n",
    "-> OFF Heap Memory stores the data in Memory but serialize format\n",
    "-> It is better than Disk but stores data in Serialize format and require to deserialize to use\n",
    "\n",
    "| --------------------------- | ----------------------------------- |\n",
    "| Problem                     | Recommended Fix                     |\n",
    "| --------------------------- | ----------------------------------- |\n",
    "| GC Overhead                 | Use `OFF_HEAP` or `MEMORY_ONLY_SER` |\n",
    "| Memory not enough           | Use `MEMORY_AND_DISK` instead       |\n",
    "| Need better performance     | Use `OFF_HEAP` (if enabled)         |\n",
    "| Want to reduce JVM pressure | Use serialized caching levels       |\n",
    "| --------------------------- | ----------------------------------- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10565c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b24a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15566f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0816c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-shell --master \"local[4]\" --driver-memory 1g --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=1g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0, 10000000).toDF(\"value\")\n",
    "\n",
    "df.unpersist()\n",
    "df.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)\n",
    "df.count()\n",
    "\n",
    "df.unpersist()\n",
    "df.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)\n",
    "df.count()\n",
    "\n",
    "\n",
    "df.unpersist()\n",
    "df.persist(org.apache.spark.storage.StorageLevel.OFF_HEAP)\n",
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
