{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5bbbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark NO_OFF_HEAP Sort\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','1'). \\\n",
    "    config('spark.executor.memory','512MB'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    config('spark.memory.offHeap.enabled','False'). \\\n",
    "    getOrCreate()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "441f1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e740372a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_14122'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45893960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"20\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad65488",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4eaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "\n",
    "df = spark.range(0, 100000000).withColumnRenamed(\"id\", \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592792ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'key' column to partition on\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "key_df = df.withColumn(\"key\", col(\"value\") % 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4d8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition and sort within partitions — this triggers a shuffle\n",
    "\n",
    "sorted_df = key_df.repartition(\"key\").sortWithinPartitions(\"key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6df9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample output - 17 sec to process\n",
    "\n",
    "sorted_df.write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c42d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038331a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark OFF_HEAP Sort\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','1'). \\\n",
    "    config('spark.executor.memory','512MB'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    config('spark.memory.offHeap.enabled','True'). \\\n",
    "    config('spark.memory.offHeap.size','1g'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5169ef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_13448'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22235c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"20\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ae8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = spark.range(0, 100000000).withColumnRenamed(\"id\", \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a603817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'key' column to partition on\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "key_df = df.withColumn(\"key\", col(\"value\") % 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c548c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition and sort within partitions — this triggers a shuffle\n",
    "\n",
    "sorted_df = key_df.repartition(\"key\").sortWithinPartitions(\"key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee4e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [key#4L ASC NULLS FIRST], false, 0\n",
      "+- Exchange hashpartitioning(key#4L, 20), REPARTITION, [id=#16]\n",
      "   +- *(1) Project [id#0L AS value#2L, (id#0L % 100) AS key#4L]\n",
      "      +- *(1) Range (0, 100000000, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b5c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write Action to run the flow - 13 sec\n",
    "\n",
    "sorted_df.write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd5768db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|partition_num|  count|\n",
      "+-------------+-------+\n",
      "|          198|1000000|\n",
      "|          195|3000000|\n",
      "|          192|1000000|\n",
      "|          190|2000000|\n",
      "|          185|1000000|\n",
      "|          184|1000000|\n",
      "|          182|1000000|\n",
      "|          179|1000000|\n",
      "|          177|1000000|\n",
      "|          175|2000000|\n",
      "|          174|1000000|\n",
      "|          172|1000000|\n",
      "|          169|1000000|\n",
      "|          168|3000000|\n",
      "|          167|1000000|\n",
      "|          166|2000000|\n",
      "|          164|2000000|\n",
      "|          161|1000000|\n",
      "|          159|1000000|\n",
      "|          154|1000000|\n",
      "+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition count\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit, desc\n",
    "\n",
    "part_df = sorted_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.orderBy(desc(\"partition_num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47d8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016edb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion:\n",
    "-> OFF Heap Memory stores the data in Memory but serialize format\n",
    "-> It is better than Disk but stores data in Serialize format and require to deserialize to use\n",
    "\n",
    "| --------------------------- | ----------------------------------- |\n",
    "| Problem                     | Recommended Fix                     |\n",
    "| --------------------------- | ----------------------------------- |\n",
    "| GC Overhead                 | Use `OFF_HEAP` or `MEMORY_ONLY_SER` |\n",
    "| Memory not enough           | Use `MEMORY_AND_DISK` instead       |\n",
    "| Need better performance     | Use `OFF_HEAP` (if enabled)         |\n",
    "| Want to reduce JVM pressure | Use serialized caching levels       |\n",
    "| --------------------------- | ----------------------------------- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d91b00",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00516a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00f011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97faf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf80a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f56a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-shell --master \"local[4]\" --driver-memory 1g --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=512m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9425f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0, 100000000).map(x => (x % 100, x)).toDF(\"key\", \"value\")\n",
    "\n",
    "val sorted = df.repartition($\"key\").sortWithinPartitions($\"key\") \n",
    "sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sorted = df.repartition($\"key\").sortWithinPartitions($\"key\") \n",
    "sorted.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
