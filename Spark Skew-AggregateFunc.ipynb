{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83f61e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(\"Spark dataSkew AggregateFunc\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    config('spark.executor.instances','2'). \\\n",
    "    config('spark.executor.memory','512MB'). \\\n",
    "    config('spark.executor.cores','4'). \\\n",
    "    config('spark.dynamicAllocation.enabled','False'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ea57f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "639a3bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1745651200635_11868'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58b9f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d810fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee Skew data\n",
    "schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(\"Datasets/employee_records_skew.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c586164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "| first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|      Jacob|     Stark|         Fine artist|1976-04-25|jasonortiz@exampl...|  224-695-9516x02171|358889.0|            1|\n",
      "|    Marissa|     Crane|Intelligence analyst|2000-06-24|johnsontroy@examp...|        277-928-0029|786608.0|            3|\n",
      "|     Andrea|     Davis|     Physiotherapist|1999-06-17| ihowell@example.org|          9503082950|428991.0|            3|\n",
      "|       John|     Tapia|Lecturer, further...|2001-09-23|russobarbara@exam...|    001-679-487-9525|241574.0|            9|\n",
      "|      Colin|    Holmes|     Psychotherapist|1965-06-29|fsimmons@example.org|          3232202899|320260.0|            4|\n",
      "|       Eric|      Beck|Speech and langua...|1970-10-28|johnny90@example.net|+1-744-277-5637x0...|125977.0|            4|\n",
      "|      Jason|    Robles|Sound technician,...|1967-03-21|jessicathomas@exa...|    001-872-480-5161|447336.0|            2|\n",
      "|   Courtney|    Daniel|     Careers adviser|1994-11-14| gmurray@example.org|  (415)357-1570x5493|514742.0|            2|\n",
      "|  Stephanie|   Morales|         Pathologist|1982-09-24|michael68@example...|001-311-431-6117x...| 14207.0|            6|\n",
      "|   Virginia| Cervantes|Chief Operating O...|1994-12-10|nicholasdavis@exa...|     +1-807-442-2027|756630.0|           10|\n",
      "|     Elijah|    Glover|Therapist, speech...|1969-10-20|brookefrederick@e...|        434.735.9681|611461.0|            8|\n",
      "|   Jennifer|   Collins|       Site engineer|1975-09-11|joseph06@example.org|     +1-416-797-0488| 19458.0|            4|\n",
      "|  Christina|      Ball|Teacher, special ...|1962-12-20| fwatson@example.org|    280.582.7025x086|709984.0|            9|\n",
      "|       Drew|  Erickson|Psychologist, edu...|1996-11-05| jason18@example.com|001-271-662-7827x...|304998.0|            3|\n",
      "|      James|    Wright|Air traffic contr...|1975-10-21|   arice@example.com|       (721)398-6804|403612.0|            4|\n",
      "|    Valerie|     Smith|      Phytotherapist|1985-04-14|frederickjames@ex...|   498.939.5081x5501|181849.0|            6|\n",
      "|Christopher|     Smith|Accountant, chart...|1970-04-01|robert20@example.net|        559-309-6778| 96578.0|            1|\n",
      "|    Patrick|     Bowen| Exhibition designer|1968-01-02|   tbell@example.net|   863-759-4636x5870|276783.0|            6|\n",
      "|      Erika|Cunningham|           Mudlogger|1974-07-02|lauren19@example.com|    688-359-8811x177|647089.0|            6|\n",
      "|    Melissa|    Murray| Librarian, academic|1985-06-14|christinaruiz@exa...|        841.594.5213| 48231.0|            5|\n",
      "+-----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ff0f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the skew for department_id=2\n",
    "\n",
    "emp_df = emp\n",
    "         \n",
    "#emp_df = emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e98e3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, row_number, avg, sum, min, max\n",
    "\n",
    "# Aggregate function: Max salary by each department\n",
    "\n",
    "df_max_sal_by_dept = emp_df.groupBy(\"department_id\").agg(max(\"salary\").alias(\"max_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f7c3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to noop - takes 4 mints\n",
    "\n",
    "df_max_sal_by_dept.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d45c269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|partition_num| count|\n",
      "+-------------+------+\n",
      "|           10|  8713|\n",
      "|            9| 60703|\n",
      "|            8| 65250|\n",
      "|            7| 65699|\n",
      "|            6|217684|\n",
      "|            5|130421|\n",
      "|            4|130312|\n",
      "|            3|130393|\n",
      "|            2|130400|\n",
      "|            1|130384|\n",
      "|            0|130406|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition count\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit, desc\n",
    "\n",
    "part_df = emp_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.orderBy(desc(\"partition_num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dffe6f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|department_id|count(1)|\n",
      "+-------------+--------+\n",
      "|            1|   99451|\n",
      "|            6|   99706|\n",
      "|            3|  100248|\n",
      "|            5|  200420|\n",
      "|            9|  100014|\n",
      "|            4|  100214|\n",
      "|            8|  100417|\n",
      "|            7|   99805|\n",
      "|           10|   99780|\n",
      "|            2|  200310|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp_df.groupBy(\"department_id\").agg(count(lit(1))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0600ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, rand, floor\n",
    "\n",
    "#salting\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31f9d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4538e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a salted key (simulate evenly spreading records)\n",
    "\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31d8be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate on salted department_id\n",
    "\n",
    "df_max_sal_by_dept = salted_emp.groupBy(\"department_id\",\"salted_dept_id\").agg(max(\"salary\").alias(\"max_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "595967b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate on department_id\n",
    "\n",
    "df_max_sal_by_dept1 = df_max_sal_by_dept.groupBy(\"department_id\").agg(max(\"max_salary\").alias(\"final_max_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0dfbf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action\n",
    "\n",
    "df_max_sal_by_dept1.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a74ad88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop sparkSession\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "569ddcf4",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. understand data skew in Aggregation or window functions\n",
    "2. understand data spill to memory and Disk\n",
    "3. how to prepare the salted key\n",
    "4. how to reduce spills using salting approch\n",
    "5. AQE can't handle the Aggregate or WIndow functions \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
